{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model for Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy import sparse\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r', encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split(' ', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples2(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r', encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split('\\t', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = English()\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(examples, feature_vocab):\n",
    "    new_examples = sparse.lil_matrix((len(examples), len(feature_vocab)))\n",
    "    for idx, example in enumerate(examples):\n",
    "        for feat in example:\n",
    "            if feat in feature_vocab:\n",
    "                new_examples[idx, feature_vocab[feat]] = example[feat]\n",
    "                \n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2features(trainX, feature_functions, tokenizer):\n",
    "    examples = []\n",
    "    count = 0\n",
    "    for doc in trainX:\n",
    "        feats = {}\n",
    "\n",
    "        tokens = tokenizer(doc)\n",
    "        \n",
    "        for func in feature_functions:\n",
    "            add_features(feats, func(tokens))\n",
    "\n",
    "        examples.append(feats)\n",
    "        count +=1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print('Processed %d examples into features' % len(examples))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pipeline for the Logistic Regression classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train LR model.\n",
    "    lr_model = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'lbfgs', max_iter = 1000)\n",
    "    lr_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test LR model.\n",
    "    print('Accuracy: %.3f' % lr_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(tokens):\n",
    "    feats = {}\n",
    "    for word in tokens:\n",
    "        feat = 'WORD_%s' % word\n",
    "        if feat in feats:\n",
    "            feats[feat] +=1\n",
    "        else:\n",
    "            feats[feat] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Instagram Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: {'spam': 120, 'ham': 124}\n",
      "Development examples: {'spam': 12, 'ham': 11}\n",
      "Test examples: {'spam': 11, 'ham': 11}\n"
     ]
    }
   ],
   "source": [
    "datapath = \"Datasets/instagram/\"\n",
    "\n",
    "train_file = os.path.join(datapath, 'train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "print(\"Training examples:\", label_counts(trainY))\n",
    "\n",
    "dev_file = os.path.join(datapath, 'dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "print(\"Development examples:\", label_counts(devY))\n",
    "\n",
    "\n",
    "test_file = os.path.join(datapath, 'test.txt')\n",
    "testX, testY = read_examples(test_file)\n",
    "print(\"Test examples:\", label_counts(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Size: 1302\n",
      "Accuracy: 0.783\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [word_features]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: {'ham': 2385, 'spam': 476}\n",
      "Development examples: {'ham': 49, 'spam': 10}\n",
      "Test examples: {'spam': 19, 'ham': 68}\n"
     ]
    }
   ],
   "source": [
    "datapath = \"Datasets/email/\"\n",
    "\n",
    "train_file = os.path.join(datapath, 'train.txt')\n",
    "trainX, trainY = read_examples2(train_file)\n",
    "print(\"Training examples:\", label_counts(trainY))\n",
    "\n",
    "dev_file = os.path.join(datapath, 'dev.txt')\n",
    "devX, devY = read_examples2(dev_file)\n",
    "print(\"Development examples:\", label_counts(devY))\n",
    "\n",
    "\n",
    "test_file = os.path.join(datapath, 'test.txt')\n",
    "testX, testY = read_examples2(test_file)\n",
    "print(\"Test examples:\", label_counts(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Processed 1600 examples into features\n",
      "Processed 1700 examples into features\n",
      "Processed 1800 examples into features\n",
      "Processed 1900 examples into features\n",
      "Processed 2000 examples into features\n",
      "Processed 2100 examples into features\n",
      "Processed 2200 examples into features\n",
      "Processed 2300 examples into features\n",
      "Processed 2400 examples into features\n",
      "Processed 2500 examples into features\n",
      "Processed 2600 examples into features\n",
      "Processed 2700 examples into features\n",
      "Processed 2800 examples into features\n",
      "Size: 32803\n",
      "Accuracy: 0.966\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [word_features]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for SMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: {'ham': 3891, 'spam': 609}\n",
      "Development examples: {'spam': 37, 'ham': 221}\n",
      "Test examples: {'spam': 27, 'ham': 169}\n"
     ]
    }
   ],
   "source": [
    "datapath = \"Datasets/sms/\"\n",
    "\n",
    "train_file = os.path.join(datapath, 'train.txt')\n",
    "trainX, trainY = read_examples2(train_file)\n",
    "print(\"Training examples:\", label_counts(trainY))\n",
    "\n",
    "dev_file = os.path.join(datapath, 'dev2.txt')\n",
    "devX, devY = read_examples2(dev_file)\n",
    "print(\"Development examples:\", label_counts(devY))\n",
    "\n",
    "test_file = os.path.join(datapath, 'test.txt')\n",
    "testX, testY = read_examples2(test_file)\n",
    "print(\"Test examples:\", label_counts(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Processed 1600 examples into features\n",
      "Processed 1700 examples into features\n",
      "Processed 1800 examples into features\n",
      "Processed 1900 examples into features\n",
      "Processed 2000 examples into features\n",
      "Processed 2100 examples into features\n",
      "Processed 2200 examples into features\n",
      "Processed 2300 examples into features\n",
      "Processed 2400 examples into features\n",
      "Processed 2500 examples into features\n",
      "Processed 2600 examples into features\n",
      "Processed 2700 examples into features\n",
      "Processed 2800 examples into features\n",
      "Processed 2900 examples into features\n",
      "Processed 3000 examples into features\n",
      "Processed 3100 examples into features\n",
      "Processed 3200 examples into features\n",
      "Processed 3300 examples into features\n",
      "Processed 3400 examples into features\n",
      "Processed 3500 examples into features\n",
      "Processed 3600 examples into features\n",
      "Processed 3700 examples into features\n",
      "Processed 3800 examples into features\n",
      "Processed 3900 examples into features\n",
      "Processed 4000 examples into features\n",
      "Processed 4100 examples into features\n",
      "Processed 4200 examples into features\n",
      "Processed 4300 examples into features\n",
      "Processed 4400 examples into features\n",
      "Processed 4500 examples into features\n",
      "Size: 10448\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [word_features]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
